# rag-chatbot-springboot-langchain4j-azure-openai

**Retrieval-Augmented Generation (RAG)** is revolutionizing how we build intelligent applications by combining the reasoning power of Large Language Models (LLMs) with custom, domain-specific knowledge bases. This project demonstrates how to build a document-aware chatbot using:

- Spring Boot for scalable backend architecture
- LangChain4j for agentic AI orchestration
- Azure OpenAI for embedding and chat completion
- Vector stores for semantic search and retrieval

The chatbot can ingest documents (PDF, DOCX), extract meaningful content, and answer user queries using context-aware responses powered by RAG.

### Prerequisites:
- Java 25,  Maven,  Docker Compose, Azure OpenAI access (embedding + chat models)

Key Features

* ‚úÖ Automatic dependency injection using @AiService
* üí¨ Reactive streaming responses via Flux<String>
* üß† Context-aware conversation handling with ChatMemoryProvider
* üìö Integration with embedding models and vector store retrievers
* üõ†Ô∏è Customizable system message to define AI behavior

### **Setup Instructions**
1. Open the project in IntelliJ IDEA.
2. Ensure Java 23+ and Maven are installed.
3. Go through LangChain4j dependencies to pom.xml:
4. Configure your language model (e.g., Azure OpenAi) and embedding store.
5. Look at how the RagChatbotService interface with the @AiService annotation.


### **AI Chat Service Interface**
Overview
The AiChatService interface defines the contract for an AI-powered chatbot service using LangChain4j.
It is annotated with @AiService, a powerful annotation that streamlines the configuration of AI components by automatically injecting essential dependencies.

```
@AiService(
        wiringMode = AiServiceWiringMode.EXPLICIT,
        chatModel = "azureChatModel",
        streamingChatModel = "azureStreamingChatModel",
        retrievalAugmentor = "retrievalAugmentor"
)
public interface ChatAssistant {

    /*  Set clear priorities for Assistant
    **  Role: AI assistant for document-aware Q&A and chat.
        Behavior: Respond in user‚Äôs language, use uploaded document context, be concise and helpful.
        Constraints: If no document context, answer from general knowledge; if context exists, prioritize it.
        Tone: Professional, clear, and user-friendly.
    */
    @SystemMessage("""
        You are an AI-powered assistant integrated into a Spring Boot application.
        Your responsibilities:
        - Engage in natural, helpful conversation with the user.
        - Always respond in the same language as the user.
        - When documents are uploaded, use their content as the primary source for answers.
        - If no relevant context is found in the documents, fall back to your general knowledge.
        - Be concise, accurate, and avoid hallucinations.
        - If unsure, state that you are unsure rather than guessing.
        - Do not reveal system or implementation details.
    """)
    Flux<String> chat(String message);
}
```
**Annotation: @AiService**
The @AiService annotation is provided by LangChain4j and plays a central role in setting up the chatbot.
It eliminates the need for manual wiring of components by automatically injecting the following beans:

**ChatLanguageModel**
The core language model responsible for generating textual responses.

**StreamingChatLanguageModel**
Enables real-time, streaming responses for enhanced interactivity.

**EmbeddingModel**
Transforms textual input into vector representations for semantic understanding.

**EmbeddingStoreContentRetriever**
Retrieves relevant documents from the vector store based on embeddings, supporting context-aware responses.

**ChatMemoryProvider**
Maintains conversational memory to preserve context across user interactions.

This annotation significantly simplifies the setup process, allowing developers to focus on implementing business logic rather than managing infrastructure.

### RAG Workflow

![RAG Workflow](/src/main/resources/plantUMLDiagrams/RAGImplSequenceDiagram.png "This is a sample image.")


Here‚Äôs a rephrased and more polished version of your explanation of how RAG works in your implementation:

---
## üîç How RAG Works in This Implementation

>  Document Ingestion Workflow
```
1. A document is uploaded via the `/context-chatbot` endpoint.
2. The file is saved locally and transformed into a `Document` object.
3. The document is segmented into overlapping chunks.
4. Each chunk is converted into vector embeddings.
5. These embeddings are stored in a **PGVector** database for retrieval.
```
---

>   Query Handling Workflow
```
1. A user submits a question.
2. The question is embedded into a vector representation.
3. Relevant document chunks are retrieved based on similarity.
4. Retrieved context is merged with the ongoing chat history.
5. **OpenAI** generates a response using the combined context.
```
---

>  Conversation Management
```
- Chat history is maintained per user session.
- The last 10 messages are used to provide conversational context.
- Responses are streamed to the user in real-time.
```
---

##  Conclusion
This project showcases a practical, production-ready implementation of a Retrieval-Augmented Generation (RAG) chatbot using modern Java technologies. By integrating Spring Boot, LangChain4j, and OpenAi models, 
it establishes a solid foundation for building intelligent, document-aware AI applications.
* The complete codebase demonstrates how to:
    * Efficiently process and embed documents
    * Store and retrieve vector embeddings using PGVector
    * Maintain contextual chat history across sessions
    * Stream AI-generated responses in real time
    * Handle document uploads via REST endpoints
    * Seamlessly integrate with local LLMs for inference

This implementation serves as a blueprint for developers aiming to build scalable, context-rich AI assistants in Java.


